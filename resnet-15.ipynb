{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10968512,"sourceType":"datasetVersion","datasetId":6824571}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport h5py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-01T12:32:47.560285Z","iopub.execute_input":"2025-04-01T12:32:47.560562Z","iopub.status.idle":"2025-04-01T12:32:54.851670Z","shell.execute_reply.started":"2025-04-01T12:32:47.560534Z","shell.execute_reply":"2025-04-01T12:32:54.850743Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport h5py\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\nclass ElectronPhotonDataset(Dataset):\n    def __init__(self, electron_file, photon_file):\n        with h5py.File(electron_file, \"r\") as f_e:\n            electrons = torch.tensor(f_e[\"X\"][:], dtype=torch.float32) \n            electron_labels = torch.tensor(f_e[\"y\"][:], dtype=torch.long)\n        \n        with h5py.File(photon_file, \"r\") as f_p:\n            photons = torch.tensor(f_p[\"X\"][:], dtype=torch.float32) \n            photon_labels = torch.tensor(f_p[\"y\"][:], dtype=torch.long)\n\n        self.images = torch.cat((electrons, photons), dim=0) \n        self.labels = torch.cat((electron_labels, photon_labels), dim=0)\n\n        self.images = self.images.permute(0, 3, 1, 2)  \n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return self.images[idx], self.labels[idx]\n\ndataset = ElectronPhotonDataset(\"/kaggle/input/photonelectron/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5\", \"/kaggle/input/photonelectron/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5\")\n\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n# train_size = int(0.4 * len(dataset))\n# val_size = int(0.1 * len(dataset))\n# rest_size = len(dataset) - train_size - val_size\n# train_dataset, val_dataset, _ = random_split(dataset, [train_size, val_size, rest_size])\n\n# Create data loaders with multiple workers\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=8, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T15:35:00.384459Z","iopub.execute_input":"2025-04-01T15:35:00.384823Z","iopub.status.idle":"2025-04-01T15:35:20.400324Z","shell.execute_reply.started":"2025-04-01T15:35:00.384791Z","shell.execute_reply":"2025-04-01T15:35:20.399638Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"class someWildBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(someWildBlock, self).__init__()\n\n        mid_channels = out_channels // 3\n        mid_channels_last = out_channels - (mid_channels + mid_channels)\n        # self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels, mid_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU()\n        )\n        \n        # 1x1 -> 3x3 conv (to simulate 5x5 receptive field)\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(in_channels, mid_channels, kernel_size=1, bias=False),\n            nn.BatchNorm2d(mid_channels),\n            nn.SiLU(),\n            nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n            nn.BatchNorm2d(mid_channels),\n            nn.SiLU()\n        )\n        \n        # Depthwise Separable 3x3 Conv\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels, kernel_size=5, groups=in_channels, padding=2, bias=False),\n            nn.Conv2d(in_channels, mid_channels_last, kernel_size=1, bias=False),\n            nn.BatchNorm2d(mid_channels_last),\n            nn.SiLU()\n        )\n        # self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n\n        \n        self.bn = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.silu = nn.SiLU()\n        \n        self.shortcut = nn.Sequential()\n        if in_channels != out_channels or stride != 1:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n\n    def forward(self, x):\n        out1 = self.conv1(x)\n        out2 = self.conv2(x)\n        out3 = self.conv3(x)\n        # print(out1.shape, out2.shape, out3.shape)\n        target_size = out1.shape[2:]  \n        out2 = F.interpolate(out2, size=target_size, mode='bilinear', align_corners=True)\n        out3 = F.interpolate(out3, size=target_size, mode='bilinear', align_corners=True)\n        \n        out = torch.cat([out1, out2, out3], dim=1)\n        out = self.bn(out)\n        \n        out = out+self.shortcut(x)\n        # out = self.relu(out)\n        # out = self.silu(out)\n        return F.silu(out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T17:11:39.927993Z","iopub.execute_input":"2025-04-01T17:11:39.928349Z","iopub.status.idle":"2025-04-01T17:11:39.937061Z","shell.execute_reply.started":"2025-04-01T17:11:39.928326Z","shell.execute_reply":"2025-04-01T17:11:39.936275Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"class ResNet15(nn.Module):\n    def __init__(self, num_classes=2, drop=0.4):\n        super(ResNet15, self).__init__()\n        self.in_channels = 64\n\n        # self.conv1 = nn.Conv2d(2, self.in_channels, kernel_size=3, stride=1, padding=1)\n        # self.conv1 = nn.Conv2d(2, self.in_channels, kernel_size=7, stride=2, padding=3)\n        self.conv1 = nn.Conv2d(2, self.in_channels, kernel_size=5, stride=1, padding=2)\n\n        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.bn = nn.BatchNorm2d(self.in_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.silu = nn.SiLU()\n\n        self.layer1 = self.block_layer(self.in_channels, 64, 2)\n        self.layer2 = self.block_layer(64, 128, 2, stride=2)\n        self.layer3 = self.block_layer(128, 256, 1, stride=2)\n        self.layer4 = self.block_layer(256, 512, 1, stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n\n        self.fc = nn.Sequential(\n                    nn.Linear(512, 256),\n                    nn.ReLU(),\n                    # nn.Dropout(0.4),\n                    nn.Dropout(drop),\n                    nn.Linear(256, num_classes)\n                )\n        \n        self.shortcut_conv = nn.Conv2d(64, 512, kernel_size=1, stride=4, bias=False)\n        self.shortcut_bn = nn.BatchNorm2d(512)\n\n    def block_layer(self, in_channels, out_channels, blocks, stride=1):\n        strides = [stride] + [1] * (blocks - 1) \n        layers = []\n        for s in strides:\n            # layers.append(Block(self.in_channels, out_channels, s))\n            layers.append(someWildBlock(self.in_channels, out_channels, s))\n            self.in_channels = out_channels \n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        shortcut_connection = self.conv1(x)\n        shortcut_connection = self.shortcut_conv(shortcut_connection) \n        shortcut_connection = self.shortcut_bn(shortcut_connection)\n\n        x = self.relu(self.bn(self.conv1(x)))\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        if shortcut_connection.shape != x.shape:\n            shortcut_connection = F.interpolate(shortcut_connection, size=x.shape[2:], mode=\"bilinear\")\n\n        x = x+shortcut_connection\n        x = self.avgpool(x)\n\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T17:11:40.577176Z","iopub.execute_input":"2025-04-01T17:11:40.577477Z","iopub.status.idle":"2025-04-01T17:11:40.586597Z","shell.execute_reply.started":"2025-04-01T17:11:40.577454Z","shell.execute_reply":"2025-04-01T17:11:40.585804Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = ResNet15(num_classes=2).to(device)\nmodel = nn.DataParallel(model)\nmodel.load_state_dict(torch.load(\"/kaggle/working/resnet15_15.pth\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T17:11:51.375648Z","iopub.execute_input":"2025-04-01T17:11:51.376010Z","iopub.status.idle":"2025-04-01T17:11:51.457367Z","shell.execute_reply.started":"2025-04-01T17:11:51.375983Z","shell.execute_reply":"2025-04-01T17:11:51.456440Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-51-3fd02cc6bcb3>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"/kaggle/working/resnet15_15.pth\"))\n","output_type":"stream"},{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":51},{"cell_type":"code","source":"import torch.optim as optim\nimport torch.nn as nn\nfrom torch.optim.lr_scheduler import OneCycleLR\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = ResNet15(num_classes=2).to(device)\nmodel = nn.DataParallel(model)  \n\ncriterion = nn.CrossEntropyLoss()\n# optimizer = optim.Adam(model.parameters(), lr=0.0003)\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)\n# optimizer = FusedLAMB(model.parameters(), lr=1e-3, weight_decay=1e-4)\n# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n# scheduler = OneCycleLR(\n#     optimizer,\n#     max_lr=5e-3,\n#     steps_per_epoch=len(train_loader), \n#     total_steps=None,\n#     epochs=10,\n#     pct_start=0.3,\n#     anneal_strategy=\"cos\",  \n#     div_factor=10,  \n#     final_div_factor=100  \n# )\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=4)\n\nmax_val = 0\nnum_epochs = 20\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    correct, total = 0, 0\n\n    for X, y in train_loader:\n        X, y = X.to(device), y.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(X.float())\n        loss = criterion(outputs, y)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        correct += (predicted == y).sum().item()\n        total += y.size(0)\n\n    train_acc = 100 * correct / total\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {train_acc:.2f}%\")\n\n    # Validation phase\n    model.eval()\n    val_loss, val_correct, val_total = 0.0, 0, 0\n    with torch.no_grad():\n        for X, y in val_loader:\n            X, y = X.to(device), y.to(device)\n            outputs = model(X.float())\n            loss = criterion(outputs, y)\n\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            val_correct += (predicted == y).sum().item()\n            val_total += y.size(0)\n\n    val_acc = 100 * val_correct / val_total\n    print(f\"Validation Loss: {val_loss/len(val_loader):.4f}, Accuracy: {val_acc:.2f}%\")\n\n    if val_acc > max_val:\n        torch.save(model.state_dict(), \"resnet15.pth\")\n        max_val = val_acc\n\n    # After 10 epochs, re-instantiate the model with best parameters and drop=0.8\n    if epoch + 1 == 10:\n        print(\"Re-instantiating model with best parameters and drop=0.8\")\n        best_model = ResNet15(num_classes=2, drop=0.8).to(device)\n        best_model.load_state_dict(torch.load(\"resnet15.pth\"))\n        model = best_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T17:10:15.026880Z","iopub.execute_input":"2025-04-01T17:10:15.027207Z","iopub.status.idle":"2025-04-01T17:10:15.032378Z","shell.execute_reply.started":"2025-04-01T17:10:15.027185Z","shell.execute_reply":"2025-04-01T17:10:15.031533Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Epoch 1/5, Loss: 0.5018, Accuracy: 76.01%  \nValidation Loss: 0.5206, Accuracy: 74.52%  \nEpoch 2/5, Loss: 0.4859, Accuracy: 76.86%  \nValidation Loss: 0.5282, Accuracy: 74.43%  \nEpoch 3/5, Loss: 0.4668, Accuracy: 77.79%  \nValidation Loss: 0.5512, Accuracy: 73.77%  \nEpoch 4/5, Loss: 0.4443, Accuracy: 77.02%  \nValidation Loss: 0.6275, Accuracy: 69.09%  \nEpoch 5/5, Loss: 0.4229, Accuracy: 78.01%  \nValidation Loss: 0.6937, Accuracy: 74.72%\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"import torch\n\n# Load the saved model state\nmodel_path = \"/kaggle/working/resnet15_15.pth\"\nmodel.load_state_dict(torch.load(model_path))\nmodel.eval()\n\ncorrect, total = 0, 0\nwith torch.no_grad():\n    for X, y in train_loader:\n        X, y = X.to(device), y.to(device)\n        outputs = model(X.float())\n        _, predicted = torch.max(outputs, 1)\n        correct += (predicted == y).sum().item()\n        total += y.size(0)\n\ntrain_acc = 100 * correct / total\nprint(f\"Training Accuracy: {train_acc:.2f}%\")\n\n# Evaluate on validation set\ncorrect, total = 0, 0\nwith torch.no_grad():\n    for X, y in val_loader:\n        X, y = X.to(device), y.to(device)\n        outputs = model(X.float())\n        _, predicted = torch.max(outputs, 1)\n        correct += (predicted == y).sum().item()\n        total += y.size(0)\n\nval_acc = 100 * correct / total\nprint(f\"Validation Accuracy: {val_acc:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T17:13:46.215146Z","iopub.execute_input":"2025-04-01T17:13:46.215530Z","iopub.status.idle":"2025-04-01T17:17:42.113766Z","shell.execute_reply.started":"2025-04-01T17:13:46.215503Z","shell.execute_reply":"2025-04-01T17:17:42.112640Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-53-b3bed70f3be2>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path))\n","output_type":"stream"},{"name":"stdout","text":"Training Accuracy: 77.90%\nValidation Accuracy: 74.72%\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"import torch\n\n# Load the state dictionary from the .pth file\nstate_dict = torch.load(\"resnet15_15.pth\", map_location=\"cpu\")\n\n# Iterate through all parameters in the state dict\nfor name, param in state_dict.items():\n    print(f\"Parameter: {name}\")\n    print(f\"Shape: {param.shape}\")\n    # Print a summary: first 5 elements flattened (if tensor has enough elements)\n    flattened = param.view(-1)\n    num_elements = flattened.numel()\n    sample = flattened[:5] if num_elements >= 5 else flattened\n    print(f\"Sample values: {sample}\")\n    print(\"-\" * 50)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T17:11:56.193770Z","iopub.execute_input":"2025-04-01T17:11:56.194110Z","iopub.status.idle":"2025-04-01T17:11:56.465054Z","shell.execute_reply.started":"2025-04-01T17:11:56.194084Z","shell.execute_reply":"2025-04-01T17:11:56.464158Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-52-d9ec1140591d>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(\"resnet15_15.pth\", map_location=\"cpu\")\n","output_type":"stream"},{"name":"stdout","text":"Parameter: module.conv1.weight\nShape: torch.Size([64, 2, 5, 5])\nSample values: tensor([0.0567, 0.0706, 0.0925, 0.0081, 0.2211])\n--------------------------------------------------\nParameter: module.conv1.bias\nShape: torch.Size([64])\nSample values: tensor([-0.1172, -0.1344,  0.0044, -0.1296,  0.0683])\n--------------------------------------------------\nParameter: module.bn.weight\nShape: torch.Size([64])\nSample values: tensor([0.8612, 0.8424, 0.9448, 1.1179, 1.0341])\n--------------------------------------------------\nParameter: module.bn.bias\nShape: torch.Size([64])\nSample values: tensor([ 0.0328, -0.0451,  0.1046,  0.1325,  0.1172])\n--------------------------------------------------\nParameter: module.bn.running_mean\nShape: torch.Size([64])\nSample values: tensor([-0.1182, -0.1333,  0.0052, -0.1304,  0.0690])\n--------------------------------------------------\nParameter: module.bn.running_var\nShape: torch.Size([64])\nSample values: tensor([0.0014, 0.0012, 0.0007, 0.0010, 0.0009])\n--------------------------------------------------\nParameter: module.bn.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\nParameter: module.layer1.0.conv1.0.weight\nShape: torch.Size([21, 64, 3, 3])\nSample values: tensor([ 0.0678, -0.0111, -0.0447, -0.0999, -0.0295])\n--------------------------------------------------\nParameter: module.layer1.0.conv1.1.weight\nShape: torch.Size([21])\nSample values: tensor([1.0717, 0.9685, 1.0000, 1.0144, 0.9450])\n--------------------------------------------------\nParameter: module.layer1.0.conv1.1.bias\nShape: torch.Size([21])\nSample values: tensor([-0.0471,  0.0429, -0.0007,  0.0255,  0.0751])\n--------------------------------------------------\nParameter: module.layer1.0.conv1.1.running_mean\nShape: torch.Size([21])\nSample values: tensor([-0.6151, -0.2400,  0.0420, -0.5410,  0.2410])\n--------------------------------------------------\nParameter: module.layer1.0.conv1.1.running_var\nShape: torch.Size([21])\nSample values: tensor([1.2751, 2.6028, 1.0324, 1.0716, 1.8215])\n--------------------------------------------------\nParameter: module.layer1.0.conv1.1.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\nParameter: module.layer1.0.conv2.0.weight\nShape: torch.Size([21, 64, 1, 1])\nSample values: tensor([-0.0849, -0.0945, -0.0929, -0.1659, -0.0707])\n--------------------------------------------------\nParameter: module.layer1.0.conv2.1.weight\nShape: torch.Size([21])\nSample values: tensor([1.0664, 1.0111, 0.9032, 1.0380, 1.0032])\n--------------------------------------------------\nParameter: module.layer1.0.conv2.1.bias\nShape: torch.Size([21])\nSample values: tensor([0.2068, 0.0417, 0.0171, 0.0863, 0.0928])\n--------------------------------------------------\nParameter: module.layer1.0.conv2.1.running_mean\nShape: torch.Size([21])\nSample values: tensor([ 0.1674, -0.1302, -0.1755, -0.2018,  0.0157])\n--------------------------------------------------\nParameter: module.layer1.0.conv2.1.running_var\nShape: torch.Size([21])\nSample values: tensor([0.1176, 0.0997, 0.1647, 0.1928, 0.0892])\n--------------------------------------------------\nParameter: module.layer1.0.conv2.1.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\nParameter: module.layer1.0.conv2.3.weight\nShape: torch.Size([21, 21, 3, 3])\nSample values: tensor([ 0.0582,  0.0386,  0.1250, -0.0745, -0.1194])\n--------------------------------------------------\nParameter: module.layer1.0.conv2.4.weight\nShape: torch.Size([21])\nSample values: tensor([1.0706, 0.9850, 1.0663, 1.0765, 0.9812])\n--------------------------------------------------\nParameter: module.layer1.0.conv2.4.bias\nShape: torch.Size([21])\nSample values: tensor([0.1133, 0.0513, 0.0182, 0.1274, 0.1383])\n--------------------------------------------------\nParameter: module.layer1.0.conv2.4.running_mean\nShape: torch.Size([21])\nSample values: tensor([ 0.1321, -0.4139, -0.6407, -0.7042, -0.4328])\n--------------------------------------------------\nParameter: module.layer1.0.conv2.4.running_var\nShape: torch.Size([21])\nSample values: tensor([0.6322, 0.5256, 0.6723, 0.3409, 0.6286])\n--------------------------------------------------\nParameter: module.layer1.0.conv2.4.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\nParameter: module.layer1.0.conv3.0.weight\nShape: torch.Size([64, 1, 5, 5])\nSample values: tensor([ 0.1566,  0.1076, -0.1514,  0.0564, -0.1574])\n--------------------------------------------------\nParameter: module.layer1.0.conv3.1.weight\nShape: torch.Size([22, 64, 1, 1])\nSample values: tensor([ 0.0035,  0.0299, -0.1219,  0.0397,  0.0167])\n--------------------------------------------------\nParameter: module.layer1.0.conv3.2.weight\nShape: torch.Size([22])\nSample values: tensor([1.1031, 1.0563, 1.0659, 0.9656, 1.0681])\n--------------------------------------------------\nParameter: module.layer1.0.conv3.2.bias\nShape: torch.Size([22])\nSample values: tensor([ 0.0697,  0.0275,  0.0418,  0.0076, -0.1506])\n--------------------------------------------------\nParameter: module.layer1.0.conv3.2.running_mean\nShape: torch.Size([22])\nSample values: tensor([-0.0849,  0.0554,  0.2112, -0.1575, -0.0878])\n--------------------------------------------------\nParameter: module.layer1.0.conv3.2.running_var\nShape: torch.Size([22])\nSample values: tensor([0.0463, 0.0560, 0.0532, 0.0831, 0.1043])\n--------------------------------------------------\nParameter: module.layer1.0.conv3.2.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\nParameter: module.layer1.0.bn.weight\nShape: torch.Size([64])\nSample values: tensor([0.9272, 0.8744, 0.8802, 1.0177, 1.0506])\n--------------------------------------------------\nParameter: module.layer1.0.bn.bias\nShape: torch.Size([64])\nSample values: tensor([ 0.0185, -0.0287, -0.0029,  0.0733,  0.1969])\n--------------------------------------------------\nParameter: module.layer1.0.bn.running_mean\nShape: torch.Size([64])\nSample values: tensor([0.1534, 0.1994, 0.2022, 0.1602, 0.2108])\n--------------------------------------------------\nParameter: module.layer1.0.bn.running_var\nShape: torch.Size([64])\nSample values: tensor([0.5874, 0.8368, 0.6956, 0.4059, 0.0569])\n--------------------------------------------------\nParameter: module.layer1.0.bn.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\nParameter: module.layer1.1.conv1.0.weight\nShape: torch.Size([21, 64, 3, 3])\nSample values: tensor([-0.0493,  0.0352, -0.0873, -0.0751,  0.0121])\n--------------------------------------------------\nParameter: module.layer1.1.conv1.1.weight\nShape: torch.Size([21])\nSample values: tensor([0.9277, 1.0819, 1.0168, 1.1100, 1.0967])\n--------------------------------------------------\nParameter: module.layer1.1.conv1.1.bias\nShape: torch.Size([21])\nSample values: tensor([-0.1032, -0.0572, -0.0446, -0.0650,  0.0340])\n--------------------------------------------------\nParameter: module.layer1.1.conv1.1.running_mean\nShape: torch.Size([21])\nSample values: tensor([ 0.0865,  0.2271, -0.2253,  0.2577, -1.2720])\n--------------------------------------------------\nParameter: module.layer1.1.conv1.1.running_var\nShape: torch.Size([21])\nSample values: tensor([ 4.9345,  6.7113,  6.1242,  7.1274, 16.3951])\n--------------------------------------------------\nParameter: module.layer1.1.conv1.1.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\nParameter: module.layer1.1.conv2.0.weight\nShape: torch.Size([21, 64, 1, 1])\nSample values: tensor([-0.0884, -0.0849,  0.1053, -0.0959,  0.0203])\n--------------------------------------------------\nParameter: module.layer1.1.conv2.1.weight\nShape: torch.Size([21])\nSample values: tensor([1.0777, 1.0977, 0.9982, 0.9316, 0.9873])\n--------------------------------------------------\nParameter: module.layer1.1.conv2.1.bias\nShape: torch.Size([21])\nSample values: tensor([0.1614, 0.0629, 0.1262, 0.0203, 0.1064])\n--------------------------------------------------\nParameter: module.layer1.1.conv2.1.running_mean\nShape: torch.Size([21])\nSample values: tensor([-0.0741, -0.2903, -0.1003, -0.2473, -0.0216])\n--------------------------------------------------\nParameter: module.layer1.1.conv2.1.running_var\nShape: torch.Size([21])\nSample values: tensor([0.9581, 0.9211, 0.5465, 1.2174, 0.4664])\n--------------------------------------------------\nParameter: module.layer1.1.conv2.1.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\nParameter: module.layer1.1.conv2.3.weight\nShape: torch.Size([21, 21, 3, 3])\nSample values: tensor([ 0.0460,  0.0849, -0.0652,  0.0018,  0.0549])\n--------------------------------------------------\nParameter: module.layer1.1.conv2.4.weight\nShape: torch.Size([21])\nSample values: tensor([1.1288, 1.1708, 1.0301, 1.0578, 1.0253])\n--------------------------------------------------\nParameter: module.layer1.1.conv2.4.bias\nShape: torch.Size([21])\nSample values: tensor([-0.0091, -0.0814,  0.0540,  0.0247,  0.0891])\n--------------------------------------------------\nParameter: module.layer1.1.conv2.4.running_mean\nShape: torch.Size([21])\nSample values: tensor([-0.4617,  0.0075, -0.2265, -0.1332, -0.3552])\n--------------------------------------------------\nParameter: module.layer1.1.conv2.4.running_var\nShape: torch.Size([21])\nSample values: tensor([1.2757, 0.6695, 0.7061, 0.2909, 0.4859])\n--------------------------------------------------\nParameter: module.layer1.1.conv2.4.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\nParameter: module.layer1.1.conv3.0.weight\nShape: torch.Size([64, 1, 5, 5])\nSample values: tensor([ 0.0207, -0.1965,  0.1996, -0.1441,  0.1561])\n--------------------------------------------------\nParameter: module.layer1.1.conv3.1.weight\nShape: torch.Size([22, 64, 1, 1])\nSample values: tensor([ 0.0006, -0.0632, -0.0147,  0.0943,  0.0861])\n--------------------------------------------------\nParameter: module.layer1.1.conv3.2.weight\nShape: torch.Size([22])\nSample values: tensor([1.0291, 1.0151, 0.9685, 1.0523, 0.9909])\n--------------------------------------------------\nParameter: module.layer1.1.conv3.2.bias\nShape: torch.Size([22])\nSample values: tensor([ 0.0123, -0.0136,  0.0161,  0.0332,  0.0359])\n--------------------------------------------------\nParameter: module.layer1.1.conv3.2.running_mean\nShape: torch.Size([22])\nSample values: tensor([-0.1319,  0.0357, -0.0870,  0.1645, -0.0702])\n--------------------------------------------------\nParameter: module.layer1.1.conv3.2.running_var\nShape: torch.Size([22])\nSample values: tensor([0.2476, 0.1913, 0.3019, 0.2479, 0.2844])\n--------------------------------------------------\nParameter: module.layer1.1.conv3.2.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\nParameter: module.layer1.1.bn.weight\nShape: torch.Size([64])\nSample values: tensor([0.9111, 0.8609, 0.8340, 0.9906, 0.9792])\n--------------------------------------------------\nParameter: module.layer1.1.bn.bias\nShape: torch.Size([64])\nSample values: tensor([ 0.0127, -0.0449, -0.0279,  0.0317,  0.1010])\n--------------------------------------------------\nParameter: module.layer1.1.bn.running_mean\nShape: torch.Size([64])\nSample values: tensor([0.1699, 0.1572, 0.1554, 0.1902, 0.2506])\n--------------------------------------------------\nParameter: module.layer1.1.bn.running_var\nShape: torch.Size([64])\nSample values: tensor([0.5448, 0.8668, 0.7793, 0.8500, 0.0391])\n--------------------------------------------------\nParameter: module.layer1.1.bn.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\nParameter: module.layer2.0.conv1.0.weight\nShape: torch.Size([42, 64, 3, 3])\nSample values: tensor([ 0.0493, -0.0011, -0.0004,  0.0068,  0.0353])\n--------------------------------------------------\nParameter: module.layer2.0.conv1.1.weight\nShape: torch.Size([42])\nSample values: tensor([0.9385, 0.9597, 0.9425, 1.1047, 1.1854])\n--------------------------------------------------\nParameter: module.layer2.0.conv1.1.bias\nShape: torch.Size([42])\nSample values: tensor([-0.0939, -0.0553, -0.0891, -0.0012, -0.0163])\n--------------------------------------------------\nParameter: module.layer2.0.conv1.1.running_mean\nShape: torch.Size([42])\nSample values: tensor([ 0.4212, -0.7612, -0.5890, -0.8622, -0.0754])\n--------------------------------------------------\nParameter: module.layer2.0.conv1.1.running_var\nShape: torch.Size([42])\nSample values: tensor([15.1352, 15.4180, 11.9482, 30.4630, 13.0095])\n--------------------------------------------------\nParameter: module.layer2.0.conv1.1.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\nParameter: module.layer2.0.conv2.0.weight\nShape: torch.Size([42, 64, 1, 1])\nSample values: tensor([ 0.0434, -0.0326, -0.0693, -0.0474, -0.0612])\n--------------------------------------------------\nParameter: module.layer2.0.conv2.1.weight\nShape: torch.Size([42])\nSample values: tensor([1.0599, 1.0286, 1.0107, 1.0593, 0.9330])\n--------------------------------------------------\nParameter: module.layer2.0.conv2.1.bias\nShape: torch.Size([42])\nSample values: tensor([0.0758, 0.1263, 0.0340, 0.1394, 0.0714])\n--------------------------------------------------\nParameter: module.layer2.0.conv2.1.running_mean\nShape: torch.Size([42])\nSample values: tensor([-0.2041, -0.1194, -0.1782, -0.1265, -0.1282])\n--------------------------------------------------\nParameter: module.layer2.0.conv2.1.running_var\nShape: torch.Size([42])\nSample values: tensor([1.1188, 1.2277, 1.3280, 1.2096, 0.9898])\n--------------------------------------------------\nParameter: module.layer2.0.conv2.1.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\nParameter: module.layer2.0.conv2.3.weight\nShape: torch.Size([42, 42, 3, 3])\nSample values: tensor([ 0.0274, -0.1010, -0.0363,  0.0461, -0.0184])\n--------------------------------------------------\nParameter: module.layer2.0.conv2.4.weight\nShape: torch.Size([42])\nSample values: tensor([1.0455, 0.9737, 1.1273, 1.0460, 0.9566])\n--------------------------------------------------\nParameter: module.layer2.0.conv2.4.bias\nShape: torch.Size([42])\nSample values: tensor([ 0.0123, -0.0004,  0.0952, -0.1097, -0.0654])\n--------------------------------------------------\nParameter: module.layer2.0.conv2.4.running_mean\nShape: torch.Size([42])\nSample values: tensor([-0.4339, -0.3962,  0.1244, -0.0407, -0.3040])\n--------------------------------------------------\nParameter: module.layer2.0.conv2.4.running_var\nShape: torch.Size([42])\nSample values: tensor([0.8339, 0.8098, 0.4678, 0.7079, 0.5094])\n--------------------------------------------------\nParameter: module.layer2.0.conv2.4.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\nParameter: module.layer2.0.conv3.0.weight\nShape: torch.Size([64, 1, 5, 5])\nSample values: tensor([-0.2140,  0.1890,  0.1666, -0.1711,  0.0052])\n--------------------------------------------------\nParameter: module.layer2.0.conv3.1.weight\nShape: torch.Size([44, 64, 1, 1])\nSample values: tensor([-0.0969, -0.0363, -0.0469, -0.0330, -0.0251])\n--------------------------------------------------\nParameter: module.layer2.0.conv3.2.weight\nShape: torch.Size([44])\nSample values: tensor([1.0625, 1.0874, 1.0404, 0.9784, 1.0479])\n--------------------------------------------------\nParameter: module.layer2.0.conv3.2.bias\nShape: torch.Size([44])\nSample values: tensor([-0.0135, -0.0848, -0.0344,  0.0504, -0.0017])\n--------------------------------------------------\nParameter: module.layer2.0.conv3.2.running_mean\nShape: torch.Size([44])\nSample values: tensor([-0.3368, -0.2644,  0.0393, -0.3327, -0.4046])\n--------------------------------------------------\nParameter: module.layer2.0.conv3.2.running_var\nShape: torch.Size([44])\nSample values: tensor([0.5679, 0.7683, 0.5941, 1.9630, 0.6515])\n--------------------------------------------------\nParameter: module.layer2.0.conv3.2.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\nParameter: module.layer2.0.bn.weight\nShape: torch.Size([128])\nSample values: tensor([0.9757, 1.0039, 0.9785, 1.0061, 0.9770])\n--------------------------------------------------\nParameter: module.layer2.0.bn.bias\nShape: torch.Size([128])\nSample values: tensor([-0.0975,  0.0789,  0.0656, -0.0484,  0.0779])\n--------------------------------------------------\nParameter: module.layer2.0.bn.running_mean\nShape: torch.Size([128])\nSample values: tensor([0.2154, 0.1445, 0.0928, 0.2101, 0.1909])\n--------------------------------------------------\nParameter: module.layer2.0.bn.running_var\nShape: torch.Size([128])\nSample values: tensor([0.4510, 0.1044, 0.1631, 0.0280, 1.0391])\n--------------------------------------------------\nParameter: module.layer2.0.bn.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\nParameter: module.layer2.0.shortcut.0.weight\nShape: torch.Size([128, 64, 1, 1])\nSample values: tensor([-0.0119,  0.0716, -0.0215,  0.0183, -0.0999])\n--------------------------------------------------\nParameter: module.layer2.0.shortcut.1.weight\nShape: torch.Size([128])\nSample values: tensor([0.8921, 1.0636, 0.9891, 0.9466, 0.9847])\n--------------------------------------------------\nParameter: module.layer2.0.shortcut.1.bias\nShape: torch.Size([128])\nSample values: tensor([-0.0975,  0.0789,  0.0656, -0.0484,  0.0779])\n--------------------------------------------------\nParameter: module.layer2.0.shortcut.1.running_mean\nShape: torch.Size([128])\nSample values: tensor([ 0.2022,  0.0661, -0.0445,  0.2532, -0.7360])\n--------------------------------------------------\nParameter: module.layer2.0.shortcut.1.running_var\nShape: torch.Size([128])\nSample values: tensor([2.2082, 1.5216, 0.8977, 1.1519, 4.3768])\n--------------------------------------------------\nParameter: module.layer2.0.shortcut.1.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\nParameter: module.layer2.1.conv1.0.weight\nShape: torch.Size([42, 128, 3, 3])\nSample values: tensor([ 0.0107,  0.0057,  0.0294,  0.0006, -0.0015])\n--------------------------------------------------\nParameter: module.layer2.1.conv1.1.weight\nShape: torch.Size([42])\nSample values: tensor([0.9546, 0.8843, 0.8127, 0.9980, 1.0216])\n--------------------------------------------------\nParameter: module.layer2.1.conv1.1.bias\nShape: torch.Size([42])\nSample values: tensor([-0.0877, -0.1298, -0.1966, -0.0539, -0.0504])\n--------------------------------------------------\nParameter: module.layer2.1.conv1.1.running_mean\nShape: torch.Size([42])\nSample values: tensor([-2.7551, -2.7689, -2.3191, -1.1097, -1.6039])\n--------------------------------------------------\nParameter: module.layer2.1.conv1.1.running_var\nShape: torch.Size([42])\nSample values: tensor([28.9504, 40.8054, 11.9185, 11.7618,  7.9331])\n--------------------------------------------------\nParameter: module.layer2.1.conv1.1.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\nParameter: module.layer2.1.conv2.0.weight\nShape: torch.Size([42, 128, 1, 1])\nSample values: tensor([ 0.0485, -0.1317,  0.0320,  0.0322,  0.0149])\n--------------------------------------------------\nParameter: module.layer2.1.conv2.1.weight\nShape: torch.Size([42])\nSample values: tensor([1.1226, 1.1196, 1.0955, 1.0740, 1.0101])\n--------------------------------------------------\nParameter: module.layer2.1.conv2.1.bias\nShape: torch.Size([42])\nSample values: tensor([-0.0008, -0.0283,  0.1054, -0.0081,  0.0377])\n--------------------------------------------------\nParameter: module.layer2.1.conv2.1.running_mean\nShape: torch.Size([42])\nSample values: tensor([-0.1125, -0.3604, -0.8763, -0.3661, -0.4149])\n--------------------------------------------------\nParameter: module.layer2.1.conv2.1.running_var\nShape: torch.Size([42])\nSample values: tensor([0.7296, 1.3632, 4.9064, 0.6531, 2.7649])\n--------------------------------------------------\nParameter: module.layer2.1.conv2.1.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\nParameter: module.layer2.1.conv2.3.weight\nShape: torch.Size([42, 42, 3, 3])\nSample values: tensor([-0.0804, -0.0243, -0.0195, -0.1158,  0.1117])\n--------------------------------------------------\nParameter: module.layer2.1.conv2.4.weight\nShape: torch.Size([42])\nSample values: tensor([1.0388, 1.0926, 1.0688, 1.0403, 1.0612])\n--------------------------------------------------\nParameter: module.layer2.1.conv2.4.bias\nShape: torch.Size([42])\nSample values: tensor([-0.1018, -0.0419, -0.0376, -0.0648,  0.0072])\n--------------------------------------------------\nParameter: module.layer2.1.conv2.4.running_mean\nShape: torch.Size([42])\nSample values: tensor([-0.9708, -0.4656, -0.4729,  0.1344, -0.2851])\n--------------------------------------------------\nParameter: module.layer2.1.conv2.4.running_var\nShape: torch.Size([42])\nSample values: tensor([0.6262, 1.3231, 0.6486, 1.6402, 0.7866])\n--------------------------------------------------\nParameter: module.layer2.1.conv2.4.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\nParameter: module.layer2.1.conv3.0.weight\nShape: torch.Size([128, 1, 5, 5])\nSample values: tensor([ 0.0922,  0.0574, -0.1800,  0.0569,  0.0761])\n--------------------------------------------------\nParameter: module.layer2.1.conv3.1.weight\nShape: torch.Size([44, 128, 1, 1])\nSample values: tensor([-0.0677, -0.0216,  0.1056,  0.0957,  0.0214])\n--------------------------------------------------\nParameter: module.layer2.1.conv3.2.weight\nShape: torch.Size([44])\nSample values: tensor([1.0285, 0.9336, 1.0435, 1.0038, 0.9637])\n--------------------------------------------------\nParameter: module.layer2.1.conv3.2.bias\nShape: torch.Size([44])\nSample values: tensor([ 0.0074, -0.1014, -0.0382, -0.1243, -0.0117])\n--------------------------------------------------\nParameter: module.layer2.1.conv3.2.running_mean\nShape: torch.Size([44])\nSample values: tensor([-0.2254,  0.2806, -0.1715,  0.1057,  0.0475])\n--------------------------------------------------\nParameter: module.layer2.1.conv3.2.running_var\nShape: torch.Size([44])\nSample values: tensor([0.3199, 0.3789, 0.2457, 0.2623, 0.2678])\n--------------------------------------------------\nParameter: module.layer2.1.conv3.2.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\nParameter: module.layer2.1.bn.weight\nShape: torch.Size([128])\nSample values: tensor([1.0940, 1.0770, 0.9560, 1.0070, 0.9683])\n--------------------------------------------------\nParameter: module.layer2.1.bn.bias\nShape: torch.Size([128])\nSample values: tensor([-0.0893,  0.0169,  0.0076, -0.0820,  0.0472])\n--------------------------------------------------\nParameter: module.layer2.1.bn.running_mean\nShape: torch.Size([128])\nSample values: tensor([0.1589, 0.1528, 0.0805, 0.1974, 0.2532])\n--------------------------------------------------\nParameter: module.layer2.1.bn.running_var\nShape: torch.Size([128])\nSample values: tensor([0.0185, 0.0153, 0.0333, 0.0997, 0.1542])\n--------------------------------------------------\nParameter: module.layer2.1.bn.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\nParameter: module.layer3.0.conv1.0.weight\nShape: torch.Size([85, 128, 3, 3])\nSample values: tensor([ 0.0370, -0.0110, -0.0103, -0.0394,  0.0370])\n--------------------------------------------------\nParameter: module.layer3.0.conv1.1.weight\nShape: torch.Size([85])\nSample values: tensor([1.0088, 0.9330, 0.9186, 1.0174, 0.9710])\n--------------------------------------------------\nParameter: module.layer3.0.conv1.1.bias\nShape: torch.Size([85])\nSample values: tensor([ 0.0099, -0.1102, -0.1111, -0.0879, -0.0892])\n--------------------------------------------------\nParameter: module.layer3.0.conv1.1.running_mean\nShape: torch.Size([85])\nSample values: tensor([-1.5764, -2.5276, -2.7457, -2.0815, -1.7803])\n--------------------------------------------------\nParameter: module.layer3.0.conv1.1.running_var\nShape: torch.Size([85])\nSample values: tensor([12.5094, 40.0072, 21.2828, 18.5355, 18.8608])\n--------------------------------------------------\nParameter: module.layer3.0.conv1.1.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\nParameter: module.layer3.0.conv2.0.weight\nShape: torch.Size([85, 128, 1, 1])\nSample values: tensor([ 0.0250, -0.0209,  0.0600, -0.0587,  0.1643])\n--------------------------------------------------\nParameter: module.layer3.0.conv2.1.weight\nShape: torch.Size([85])\nSample values: tensor([1.0578, 1.0316, 1.0961, 0.9843, 0.9899])\n--------------------------------------------------\nParameter: module.layer3.0.conv2.1.bias\nShape: torch.Size([85])\nSample values: tensor([-0.0153,  0.0045, -0.0320,  0.0428, -0.0410])\n--------------------------------------------------\nParameter: module.layer3.0.conv2.1.running_mean\nShape: torch.Size([85])\nSample values: tensor([-0.3265, -0.1461, -0.2595,  0.0805, -0.3225])\n--------------------------------------------------\nParameter: module.layer3.0.conv2.1.running_var\nShape: torch.Size([85])\nSample values: tensor([1.2185, 1.3135, 2.5012, 0.9513, 1.2324])\n--------------------------------------------------\nParameter: module.layer3.0.conv2.1.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\nParameter: module.layer3.0.conv2.3.weight\nShape: torch.Size([85, 85, 3, 3])\nSample values: tensor([0.0059, 0.0182, 0.0804, 0.0152, 0.0411])\n--------------------------------------------------\nParameter: module.layer3.0.conv2.4.weight\nShape: torch.Size([85])\nSample values: tensor([1.0741, 0.9807, 1.0872, 1.1020, 0.9732])\n--------------------------------------------------\nParameter: module.layer3.0.conv2.4.bias\nShape: torch.Size([85])\nSample values: tensor([-0.1505, -0.0879, -0.1636, -0.0015, -0.1302])\n--------------------------------------------------\nParameter: module.layer3.0.conv2.4.running_mean\nShape: torch.Size([85])\nSample values: tensor([-0.3065, -0.0297, -0.5427,  0.3975, -0.5695])\n--------------------------------------------------\nParameter: module.layer3.0.conv2.4.running_var\nShape: torch.Size([85])\nSample values: tensor([0.9912, 0.9260, 0.8783, 3.7252, 1.3013])\n--------------------------------------------------\nParameter: module.layer3.0.conv2.4.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\nParameter: module.layer3.0.conv3.0.weight\nShape: torch.Size([128, 1, 5, 5])\nSample values: tensor([ 0.1525, -0.0962,  0.2042, -0.1277, -0.1757])\n--------------------------------------------------\nParameter: module.layer3.0.conv3.1.weight\nShape: torch.Size([86, 128, 1, 1])\nSample values: tensor([ 0.0477, -0.0121,  0.1059, -0.0149, -0.0225])\n--------------------------------------------------\nParameter: module.layer3.0.conv3.2.weight\nShape: torch.Size([86])\nSample values: tensor([0.9940, 1.0282, 0.9718, 1.0409, 1.1029])\n--------------------------------------------------\nParameter: module.layer3.0.conv3.2.bias\nShape: torch.Size([86])\nSample values: tensor([-0.0395, -0.0173, -0.0162,  0.0004, -0.0519])\n--------------------------------------------------\nParameter: module.layer3.0.conv3.2.running_mean\nShape: torch.Size([86])\nSample values: tensor([-0.1141, -0.1402, -0.1002, -0.0391, -0.1819])\n--------------------------------------------------\nParameter: module.layer3.0.conv3.2.running_var\nShape: torch.Size([86])\nSample values: tensor([0.6290, 0.6030, 0.8080, 0.4894, 0.6714])\n--------------------------------------------------\nParameter: module.layer3.0.conv3.2.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\nParameter: module.layer3.0.bn.weight\nShape: torch.Size([256])\nSample values: tensor([0.9612, 1.0435, 1.0306, 0.9972, 0.9695])\n--------------------------------------------------\nParameter: module.layer3.0.bn.bias\nShape: torch.Size([256])\nSample values: tensor([-0.0587, -0.0351, -0.0192, -0.1948, -0.1158])\n--------------------------------------------------\nParameter: module.layer3.0.bn.running_mean\nShape: torch.Size([256])\nSample values: tensor([0.3078, 0.1288, 0.2340, 0.2040, 0.2596])\n--------------------------------------------------\nParameter: module.layer3.0.bn.running_var\nShape: torch.Size([256])\nSample values: tensor([0.1206, 0.0537, 0.0404, 0.0709, 0.0818])\n--------------------------------------------------\nParameter: module.layer3.0.bn.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\nParameter: module.layer3.0.shortcut.0.weight\nShape: torch.Size([256, 128, 1, 1])\nSample values: tensor([ 0.0397,  0.0596, -0.0320,  0.0527, -0.0535])\n--------------------------------------------------\nParameter: module.layer3.0.shortcut.1.weight\nShape: torch.Size([256])\nSample values: tensor([0.9252, 0.9967, 1.0443, 0.8968, 1.0858])\n--------------------------------------------------\nParameter: module.layer3.0.shortcut.1.bias\nShape: torch.Size([256])\nSample values: tensor([-0.0587, -0.0351, -0.0192, -0.1948, -0.1158])\n--------------------------------------------------\nParameter: module.layer3.0.shortcut.1.running_mean\nShape: torch.Size([256])\nSample values: tensor([ 0.1059, -0.3522, -0.8093,  0.1116, -0.5957])\n--------------------------------------------------\nParameter: module.layer3.0.shortcut.1.running_var\nShape: torch.Size([256])\nSample values: tensor([1.1349, 2.0260, 4.6500, 2.1551, 3.0864])\n--------------------------------------------------\nParameter: module.layer3.0.shortcut.1.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\nParameter: module.layer4.0.conv1.0.weight\nShape: torch.Size([170, 256, 3, 3])\nSample values: tensor([ 0.0712,  0.0305, -0.0204, -0.0080,  0.0124])\n--------------------------------------------------\nParameter: module.layer4.0.conv1.1.weight\nShape: torch.Size([170])\nSample values: tensor([0.8573, 0.9565, 0.6924, 1.0218, 0.7471])\n--------------------------------------------------\nParameter: module.layer4.0.conv1.1.bias\nShape: torch.Size([170])\nSample values: tensor([-0.1433, -0.0663, -0.3041, -0.0329, -0.2440])\n--------------------------------------------------\nParameter: module.layer4.0.conv1.1.running_mean\nShape: torch.Size([170])\nSample values: tensor([-0.4353,  0.2403, -1.6330, -2.1116, -0.6917])\n--------------------------------------------------\nParameter: module.layer4.0.conv1.1.running_var\nShape: torch.Size([170])\nSample values: tensor([11.6164, 51.5743, 20.3798, 46.0015, 11.5165])\n--------------------------------------------------\nParameter: module.layer4.0.conv1.1.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\nParameter: module.layer4.0.conv2.0.weight\nShape: torch.Size([170, 256, 1, 1])\nSample values: tensor([ 0.0831,  0.0128, -0.0430, -0.0922,  0.0256])\n--------------------------------------------------\nParameter: module.layer4.0.conv2.1.weight\nShape: torch.Size([170])\nSample values: tensor([0.9720, 0.9304, 1.0907, 1.0115, 0.9735])\n--------------------------------------------------\nParameter: module.layer4.0.conv2.1.bias\nShape: torch.Size([170])\nSample values: tensor([-0.1370, -0.0223,  0.0018, -0.0780, -0.2528])\n--------------------------------------------------\nParameter: module.layer4.0.conv2.1.running_mean\nShape: torch.Size([170])\nSample values: tensor([-0.3277,  0.2578, -0.2485,  0.1213,  0.6057])\n--------------------------------------------------\nParameter: module.layer4.0.conv2.1.running_var\nShape: torch.Size([170])\nSample values: tensor([3.2334, 2.7889, 0.9320, 0.9114, 0.8530])\n--------------------------------------------------\nParameter: module.layer4.0.conv2.1.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\nParameter: module.layer4.0.conv2.3.weight\nShape: torch.Size([170, 170, 3, 3])\nSample values: tensor([-0.0609,  0.0528, -0.0385,  0.0618,  0.0267])\n--------------------------------------------------\nParameter: module.layer4.0.conv2.4.weight\nShape: torch.Size([170])\nSample values: tensor([0.9395, 0.8955, 1.0425, 0.9930, 1.0972])\n--------------------------------------------------\nParameter: module.layer4.0.conv2.4.bias\nShape: torch.Size([170])\nSample values: tensor([-0.2440, -0.3119, -0.0817, -0.1491, -0.1824])\n--------------------------------------------------\nParameter: module.layer4.0.conv2.4.running_mean\nShape: torch.Size([170])\nSample values: tensor([-0.7057, -0.6698, -1.4583, -1.3287, -0.0806])\n--------------------------------------------------\nParameter: module.layer4.0.conv2.4.running_var\nShape: torch.Size([170])\nSample values: tensor([4.5459, 7.9293, 6.5734, 5.4766, 6.5712])\n--------------------------------------------------\nParameter: module.layer4.0.conv2.4.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\nParameter: module.layer4.0.conv3.0.weight\nShape: torch.Size([256, 1, 5, 5])\nSample values: tensor([ 0.1060, -0.0571,  0.0511, -0.0323, -0.0448])\n--------------------------------------------------\nParameter: module.layer4.0.conv3.1.weight\nShape: torch.Size([172, 256, 1, 1])\nSample values: tensor([ 0.0057, -0.0661, -0.0219,  0.0921,  0.1361])\n--------------------------------------------------\nParameter: module.layer4.0.conv3.2.weight\nShape: torch.Size([172])\nSample values: tensor([1.1031, 1.0460, 1.0168, 1.1158, 0.9203])\n--------------------------------------------------\nParameter: module.layer4.0.conv3.2.bias\nShape: torch.Size([172])\nSample values: tensor([-0.0233, -0.0986, -0.1112, -0.0321, -0.0950])\n--------------------------------------------------\nParameter: module.layer4.0.conv3.2.running_mean\nShape: torch.Size([172])\nSample values: tensor([-0.0505, -0.1247, -0.2014, -0.0011,  0.0621])\n--------------------------------------------------\nParameter: module.layer4.0.conv3.2.running_var\nShape: torch.Size([172])\nSample values: tensor([0.2579, 0.2961, 0.3398, 0.4103, 0.3764])\n--------------------------------------------------\nParameter: module.layer4.0.conv3.2.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\nParameter: module.layer4.0.bn.weight\nShape: torch.Size([512])\nSample values: tensor([0.9523, 0.8913, 1.0901, 0.9759, 1.0699])\n--------------------------------------------------\nParameter: module.layer4.0.bn.bias\nShape: torch.Size([512])\nSample values: tensor([-0.0591, -0.0136, -0.0228, -0.0231, -0.0777])\n--------------------------------------------------\nParameter: module.layer4.0.bn.running_mean\nShape: torch.Size([512])\nSample values: tensor([0.2210, 0.2108, 0.0341, 0.2417, 0.1223])\n--------------------------------------------------\nParameter: module.layer4.0.bn.running_var\nShape: torch.Size([512])\nSample values: tensor([0.2074, 0.7497, 0.0215, 0.7723, 0.1069])\n--------------------------------------------------\nParameter: module.layer4.0.bn.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\nParameter: module.layer4.0.shortcut.0.weight\nShape: torch.Size([512, 256, 1, 1])\nSample values: tensor([ 0.0754, -0.0823, -0.0671, -0.0492, -0.0128])\n--------------------------------------------------\nParameter: module.layer4.0.shortcut.1.weight\nShape: torch.Size([512])\nSample values: tensor([0.9329, 0.9924, 0.9404, 0.9592, 0.9389])\n--------------------------------------------------\nParameter: module.layer4.0.shortcut.1.bias\nShape: torch.Size([512])\nSample values: tensor([-0.0591, -0.0136, -0.0228, -0.0231, -0.0777])\n--------------------------------------------------\nParameter: module.layer4.0.shortcut.1.running_mean\nShape: torch.Size([512])\nSample values: tensor([-1.1325, -1.1673, -0.9376, -1.4920, -0.2466])\n--------------------------------------------------\nParameter: module.layer4.0.shortcut.1.running_var\nShape: torch.Size([512])\nSample values: tensor([19.6201, 27.0604, 20.3590, 27.0558,  8.9515])\n--------------------------------------------------\nParameter: module.layer4.0.shortcut.1.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\nParameter: module.fc.0.weight\nShape: torch.Size([256, 512])\nSample values: tensor([-0.0275, -0.0067, -0.1003, -0.0441, -0.1331])\n--------------------------------------------------\nParameter: module.fc.0.bias\nShape: torch.Size([256])\nSample values: tensor([-0.0664, -0.0263, -0.0171,  0.0075, -0.0021])\n--------------------------------------------------\nParameter: module.fc.3.weight\nShape: torch.Size([2, 256])\nSample values: tensor([-0.0501,  0.0054,  0.0414,  0.0044, -0.0390])\n--------------------------------------------------\nParameter: module.fc.3.bias\nShape: torch.Size([2])\nSample values: tensor([0.0280, 0.0451])\n--------------------------------------------------\nParameter: module.shortcut_conv.weight\nShape: torch.Size([512, 64, 1, 1])\nSample values: tensor([-0.0850,  0.1468, -0.0526,  0.0257,  0.1334])\n--------------------------------------------------\nParameter: module.shortcut_bn.weight\nShape: torch.Size([512])\nSample values: tensor([0.8428, 0.8337, 0.8592, 0.8007, 0.8879])\n--------------------------------------------------\nParameter: module.shortcut_bn.bias\nShape: torch.Size([512])\nSample values: tensor([-0.0479, -0.0086, -0.0142, -0.0220, -0.0741])\n--------------------------------------------------\nParameter: module.shortcut_bn.running_mean\nShape: torch.Size([512])\nSample values: tensor([-0.1223, -0.0310,  0.0651, -0.0236, -0.0008])\n--------------------------------------------------\nParameter: module.shortcut_bn.running_var\nShape: torch.Size([512])\nSample values: tensor([0.0016, 0.0010, 0.0026, 0.0030, 0.0045])\n--------------------------------------------------\nParameter: module.shortcut_bn.num_batches_tracked\nShape: torch.Size([])\nSample values: tensor([38914])\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}